---
title: 'crawling, parsing #1'
thumbnail: /image/(source/crawling/crawling_1.png)
date: 2019-05-16 12:48:28
tags:
    - Crawling
    - Parsing
    - Scraping
    - 크롤링
    - 스크래핑
    - 파싱

---

> 크롤링 시리즈
1. [크롤링은 왜 하는걸까]()
2. [꼭 Python, Node.js 로만 가능할까?]()
3. [크롤링을 하는데 알야야 할 것은?]()

---

크롤링(crawling) 또는 스크래핑(scraping)그리고 파싱(parsing)은 구분하기가 애매하다고 생각한다. 물론 파싱은 컴파일러 설계 등에서 사용되는 의미있는 토큰으로 나누어 파스트리를 만드는 파싱의 의미도 있지만 여기서는 단순히 데이터를 가공하여 정보로 만드는 과정을 말하려고 한다.

인터넷의 발달에 따라 우리는 기존의 신문, 잡지등에서 웹브라우저로, 앱으로 정보 수집의 창구가 이동하고 있다. 그리고 어떠한 정보를 가지고 있는가, 얼마나 빠르게 습득하느냐도 점차 중요해지면서 주기적으로 데이터를 수집하는 업무가 증가하고 있으며 이로 인하여 인터넷에서 데이터를 가져오는 과정을 자동화하는 크롤링이 관심을 받고 있다.

<!-- more -->

특히 요즘에는 인터넷 강의 업체에서도 크롤링을 통한 업무 자동화 등 다양한 홍보를 하고 있다. 그런데 대부분 50 ~ 80 만원 정도의 가격대를 형성하고 있는데 사실 개인적인 기준에서는 수강하기에는 좀 부담스러운 가격이긴 하다. 더군다나 대부분은 파이썬 등 프로그래밍 언어를 사용하는 강의이고 어느정도 프로그래밍이 가능해야 또는 최소한 개념은 알아야 수강할 수 있는 강의라는 얘기를 들었다.

그래서 이번에는 정리 겸 크롤링에 대해 써보려고 한다.

---

## 크롤링은 왜 하는걸까

크롤링을 하는데는 다양한 이유가 있다. 구글, 네이버 등 검색 엔진에서 자사 서비스에 검색 결과로 노출하기 위한 크롤링부터 기업이 CS 업무용 카페를 모니터링하기 위한 크롤링, 연구를 위한 데이터 수집용 크롤링, 그 외 업무에 필요한 데이터 수집용 크롤링 등 다양한 목적에서 사용되고 있다.

![이미지 설명](/crawling/crawling_1.png)

기업업무 등에서 사용되는 크롤링이던 개인적으로 쓰는 크롤링이던간에 사실 크롤링은 시간만 충분히 있다면 사람이 처리할 수 있는 작업이다.

1. 컴퓨터를 켜고
2. 해당 사이트에 접속해서
3. 원하는 페이지를 열고
4. 원하는 데이터 (제목, 내용 등)을 가져오고
5. 엑셀이나 텍스트 파일 등 필요한 형태로 저장

단순하게 이 과정만 반복하면 된다. 이 업무가 필요 없을 때 까지 말이다.

만약 회사에서 맡은 업무중 하나가 100개의 뉴스 사이트를 확인하면서 전날 새로 올라온 기사를 수집하는 업무라고 가정한다면 아마 처음 몇일은 열심히 하겠지만 다음날부터는 보기만 해도 지겹고 진절머리날 것 이다.

물론 오늘의 뉴스 헤드라인 전달 등 몇개 안되는 작업이라면 그정도 쯤이야 하면서 하는 사람도 있을지도 모른다 하지만 군대에서 매 주 기록 정리해서 보고하는 일 조차 귀찮아 했던 사람이 나다. 저런 업무를 매일 또는 주기적으로 할 수 있을리가 없다.

더군다나 해야할 일은 자료 수집만 있는것도 아니니 일이 많은 날에는 초조하기도 하고 짜증도 날 것이다.

이러한 시점에서 필요한것이 바로 ``크롤링``이다. 

크롤링용 시스템을 운영할 수 있는 기업에서는 기존에 데이터 수집에 소모되던 인력을 다른곳에 투입할 수 있게 되며 업무상 데이터 수집이 주기적으로 필요한 개인은 데이터 수집 업무에 소요되던 시간을 다른 업무나 휴식에 쓸 수 있을것이다. 개인적으로는 크롤링 작업을 자동화 해서 나쁠것은 없다고 생각한다.
